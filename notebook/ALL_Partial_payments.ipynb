{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06207bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def export_ALL_Partial_payments(year: int, month: int, \n",
    "                        out_dir: str = r\"..\\data\\All_Partial_payments\") -> tuple[pd.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Descarga, filtra y exporta a CSV los contratos premium con Abo Unique = 1.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    year : int\n",
    "        Año de la consulta (ejemplo: 2025).\n",
    "    month : int\n",
    "        Mes de la consulta en formato numérico (ejemplo: 8 o 12).\n",
    "    out_dir : str, opcional\n",
    "        Carpeta donde se guardará el archivo. Por defecto ../data/Debt_premium\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    tuple[pd.DataFrame, str]\n",
    "        - DataFrame filtrado.\n",
    "        - Ruta completa del archivo generado.\n",
    "    \"\"\"\n",
    "    # Formatear año y mes\n",
    "    year_str = str(year)\n",
    "    month_str = f\"{month:02d}\"  # asegura siempre 2 dígitos (ej: 08, 12)\n",
    "\n",
    "    base_url = \"https://billing.izoswap.fr/admin/export/payments_subs_attempts/ALL\"\n",
    "    token = os.getenv(\"PAYMENTS_TOKEN\")\n",
    "    url = f\"{base_url}/{year_str}-{month_str}/{token}/csv\"\n",
    "\n",
    "    # Descargar CSV en memoria\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    df = pd.read_csv(StringIO(response.text), sep=\";\")\n",
    "\n",
    "    # Filtro\n",
    "    mask = (df[\"Role name\"] == \"Error Subscription\") & (df['Partner Code']=='118')\n",
    "    columns = [\"UID\", \"Date Transaction\", \"Campaign Type\"]\n",
    "    df_filtered = df.loc[mask, columns]\n",
    "\n",
    "    # --- Mantener la fila más reciente por UID ---\n",
    "    # 1) Parsear fechas en formato día-mes-año (para evitar warnings)\n",
    "    df_filtered[\"__dt\"] = pd.to_datetime(\n",
    "        df_filtered[\"Date Transaction\"], errors=\"coerce\", dayfirst=True\n",
    "    )\n",
    "\n",
    "    # 2) Ordenar por UID y fecha descendente\n",
    "    df_filtered = df_filtered.sort_values([\"UID\", \"__dt\"], ascending=[True, False], na_position=\"last\")\n",
    "\n",
    "    # 3) Quitar duplicados por UID, manteniendo la fila más reciente\n",
    "    df_filtered = df_filtered.drop_duplicates(subset=[\"UID\"], keep=\"first\")\n",
    "\n",
    "    # 4) Normalizar formato de fecha a ISO estándar\n",
    "    df_filtered[\"Date Transaction\"] = df_filtered[\"__dt\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # 5) Limpiar columna auxiliar\n",
    "    df_filtered = df_filtered.drop(columns=\"__dt\")\n",
    "\n",
    "    # Crear carpeta si no existe\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Definir nombre del archivo\n",
    "    file_name = f\"All_Partial_payments{year_str}_{month_str}.csv\"\n",
    "    file_path = os.path.join(out_dir, file_name)\n",
    "\n",
    "    # Exportar\n",
    "    df_filtered.to_csv(file_path, index=False)\n",
    "\n",
    "    return df_filtered, file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdc6f178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(652, 3)\n",
      "Archivo generado en: ..\\data\\All_Partial_payments\\All_Partial_payments2025_08.csv\n"
     ]
    }
   ],
   "source": [
    "df_premium, ruta = export_ALL_Partial_payments(2025, 8)\n",
    "print(df_premium.shape)\n",
    "print(f\"Archivo generado en: {ruta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bc606bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def build_potential_All_Partial_payments_user(\n",
    "    dir_path: str = r\"..\\data\\All_Partial_payments\",\n",
    "    pattern: str = \"All_Partial_payments\",\n",
    "    clients_filename: str = \"Debt_clients.csv\",\n",
    "    out_filename: str = \"Potential_All_Partial_payments_user.csv\",\n",
    "    sep: str = \",\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Concatena CSVs cuyo nombre contenga `pattern`, agrega flag de pertenencia\n",
    "    a Debt_clients.csv y guarda el resultado en `out_filename`.\n",
    "    \"\"\"\n",
    "    base = Path(dir_path)\n",
    "    if not base.exists():\n",
    "        raise FileNotFoundError(f\"No existe la carpeta: {base}\")\n",
    "\n",
    "    # CSVs candidatos (evitar el archivo de salida y el Debt_clients)\n",
    "    csvs = sorted([\n",
    "        f for f in base.glob(\"*.csv\")\n",
    "        if pattern.lower() in f.name.lower()\n",
    "        and f.name.lower() != clients_filename.lower()\n",
    "        and out_filename.lower() not in f.name.lower()\n",
    "    ])\n",
    "    if not csvs:\n",
    "        raise FileNotFoundError(f\"No hay CSVs con '{pattern}' en {base}\")\n",
    "\n",
    "    # Leer y concatenar SIN quitar duplicados\n",
    "    dfs = [pd.read_csv(f, sep=sep, dtype={'UID': \"string\"}, low_memory=False) for f in csvs]\n",
    "    df_all = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "\n",
    "    # Leer Debt_clients.csv\n",
    "    clients_path = base / clients_filename\n",
    "    if not clients_path.exists():\n",
    "        raise FileNotFoundError(f\"No se encontró {clients_filename} en {base}\")\n",
    "    df_clients = pd.read_csv(clients_path, sep=sep, dtype={'UID': \"string\"}, low_memory=False)\n",
    "\n",
    "    # Validaciones mínimas\n",
    "    if \"UID\" not in df_all.columns:\n",
    "        raise KeyError(\"La columna 'UID' no existe en los CSV concatenados.\")\n",
    "    if \"UID\" not in df_clients.columns:\n",
    "        raise KeyError(\"La columna 'UID' no existe en Debt_clients.csv.\")\n",
    "\n",
    "    # Flag 1/0 si el UID está en Debt_clients\n",
    "    df_all[\"Potential_All_Partial_payments_user\"] = df_all[\"UID\"].isin(df_clients[\"UID\"]).astype(int)\n",
    "\n",
    "    # Guardar\n",
    "    out_path = base / out_filename\n",
    "    df_all.to_csv(out_path, index=False, sep=sep)\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ba80bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(652, 4)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso:\n",
    "df_all=build_potential_All_Partial_payments_user()\n",
    "print(df_all.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
